{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "from mask_to_submission import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from preprocessing import *\n",
    "from experimental_neural_nets import Conv_test, Road_data, UNet\n",
    "from road_correction import process_roads, f1_loss_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data/training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files))  # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "imgs,gt_imgs = rotate_train_data(imgs,gt_imgs)\n",
    "imgs,gt_imgs = flip_train_data(imgs,gt_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(base_c=8,num_layers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "dataset = Road_data(imgs,gt_imgs)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_check = Road_data(imgs, gt_imgs)\n",
    "dataloader = DataLoader(dataset_check, batch_size=400, shuffle=True)\n",
    "\n",
    "best_t = 0\n",
    "best_outlier_size = 0\n",
    "best_shape_size = 0\n",
    "best_loss = np.inf\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    print(f'batch index : {batch_idx}')\n",
    "    gt = target\n",
    "    images = data\n",
    "    pred = model(data)\n",
    "\n",
    "    gt_np = gt.detach().cpu().numpy()\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "\n",
    "    for threshold in np.linspace(0.1, 0.9, 20):\n",
    "        for outlier_size in np.arange(10, 100, 10):\n",
    "            for shape_size in np.arange(3, 10):\n",
    "\n",
    "                test_pred = np.array([\n",
    "                    process_roads(raw_map=raw_map, threshold=threshold, outlier_size=outlier_size, shape_size=shape_size) \n",
    "                    for raw_map in pred_np\n",
    "                ])\n",
    "                c_loss = f1_loss_numpy(test_pred, gt_np)\n",
    "            \n",
    "                if c_loss < best_loss:\n",
    "                    best_loss = c_loss\n",
    "                    best_t = threshold\n",
    "                    best_outlier_size = outlier_size\n",
    "                    best_shape_size = shape_size\n",
    "\n",
    "                    \n",
    "print( best_t)\n",
    "\n",
    "n_example = 5\n",
    "\n",
    "# get examples from last batch\n",
    "for i in range(min(n_example,len(pred_np))):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(6, 6))\n",
    "    prediction = process_roads(raw_map=pred_np[i], threshold=best_t, outlier_size=best_outlier_size, shape_size=best_shape_size)\n",
    "    axes[0].imshow(prediction, cmap='gray')\n",
    "    axes[0].set_title(\"Prediction\")\n",
    "    \n",
    "    axes[1].imshow(pred_np[i], cmap='gray')\n",
    "    axes[1].set_title(\"Probability Map\")\n",
    "\n",
    "    axes[2].imshow(gt[i], cmap='gray')\n",
    "    axes[2].set_title(\"Ground Truth\")\n",
    "\n",
    "    img_np = images[i].permute(1, 2, 0).cpu().numpy() \n",
    "\n",
    "    axes[3].imshow(img_np)\n",
    "    axes[3].set_title(\"Original Image\")\n",
    "\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[2].axis(\"off\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_shape_size = 8\n",
    "best_outlier_size = 10\n",
    "# best_threshold = 0.45 - 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the root directory containing the test image folders\n",
    "test_dir = \"data/test_set_images/\"\n",
    "\n",
    "# Helper function to perform natural sorting\n",
    "def natural_sort_key(s):\n",
    "    # Extract numbers from the string for sorting\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# List all subdirectories (each containing one image) and sort them naturally\n",
    "test_folders = sorted(\n",
    "    [folder for folder in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, folder))],\n",
    "    key=natural_sort_key\n",
    ")\n",
    "\n",
    "# Calculate the number of test images (based on the number of folders)\n",
    "n_test = len(test_folders)\n",
    "print(\"Loading \" + str(n_test) + \" images\")\n",
    "\n",
    "# Iterate over each folder, find the image, and load it\n",
    "test_imgs = []\n",
    "for folder in test_folders:\n",
    "    folder_path = os.path.join(test_dir, folder)\n",
    "    \n",
    "    # List all files in the folder (assuming only one image per folder) and sort them naturally\n",
    "    image_files = sorted(\n",
    "        [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))],\n",
    "        key=natural_sort_key\n",
    "    )\n",
    "    \n",
    "    # Assuming there's only one image per folder, get the image file path\n",
    "    if image_files:\n",
    "        image_path = os.path.join(folder_path, image_files[0])\n",
    "        test_imgs.append(load_image(image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = best_t\n",
    "test_imgs_np = np.array(test_imgs)\n",
    "images_test= torch.from_numpy(test_imgs_np).permute(0, 3, 1, 2)\n",
    "pred= model(images_test)\n",
    "pred_np = pred.detach().cpu().numpy()\n",
    "prediction= np.array([process_roads(raw_map=raw_map, threshold=threshold, outlier_size=best_outlier_size, shape_size=best_shape_size) for raw_map in pred_np])\n",
    "\n",
    "def getting_sub (prediction):\n",
    "\n",
    "    output_dir = 'predictions_opti'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save each prediction as an image\n",
    "    pred_filenames = []\n",
    "    for i in range(prediction.shape[0]):\n",
    "        pred_mask = (prediction[i] > threshold).astype(np.uint8)  # Example thresholding, modify as needed\n",
    "        pred_image = Image.fromarray(pred_mask * 255)  # Save the mask as a black and white image\n",
    "        filename = os.path.join(output_dir, f\"prediction_opti{i+1:03d}.png\")\n",
    "        pred_image.save(filename)\n",
    "        pred_filenames.append(filename)\n",
    "\n",
    "    # Now you can call the masks_to_submission function with the generated file paths\n",
    "    submission_filename = 'UNet_opti.csv'\n",
    "    masks_to_submission(submission_filename, *pred_filenames)\n",
    "getting_sub(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
