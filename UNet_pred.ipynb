{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from preprocessing import *\n",
    "from experimental_neural_nets import Road_data, UNet\n",
    "from road_correction import process_roads, f1_loss_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global params for UNet and Road Correction (SEARCH or LOAD, option to SAVE if SEARCH)\n",
    "UNET_SEARCH = False\n",
    "SAVE_UNET = False\n",
    "ROAD_CORRECTION_SEARCH = False\n",
    "SAVE_RC_PARAMS = False\n",
    "UNET_MODEL_LOAD = 'UNet_model.pth'\n",
    "UNET_MODEL_SAVE = 'UNet_model.pth'\n",
    "RC_LOAD = 'RC_params.pth'\n",
    "RC_SAVE = 'RC_params.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data/training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files))  # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "#imgs,gt_imgs = rotate_train_data(imgs,gt_imgs)\n",
    "#imgs,gt_imgs = flip_train_data(imgs,gt_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for best parameters of the model (base_C and num_layers) or load from save\n",
    "if UNET_SEARCH:\n",
    "\n",
    "    # Modulate search\n",
    "    num_epochs = 5\n",
    "    batch_size = 10\n",
    "    bases = [16]#[4,8,16,32]\n",
    "    layers = [4]#np.arange(3,7)\n",
    "\n",
    "    dataset = Road_data(imgs,gt_imgs)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_base = 0\n",
    "    best_n_layer = 0\n",
    "\n",
    "\n",
    "    for base_c in bases:\n",
    "        for n_layer in layers:\n",
    "\n",
    "            model = UNet(base_c=base_c,num_layers=n_layer)\n",
    "            losses = []\n",
    "            criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                losses.append(loss)\n",
    "                scheduler.step()\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "            c_loss = min(losses)\n",
    "            if c_loss < best_loss:\n",
    "                best_base = base_c\n",
    "                best_n_layer = n_layer\n",
    "                best_loss = c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training of best model or loading from save\n",
    "if UNET_SEARCH:\n",
    "\n",
    "    # Modulate training\n",
    "    num_epochs = 10\n",
    "    batch_size = 10\n",
    "\n",
    "    dataset = Road_data(imgs,gt_imgs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = UNet(base_c=best_base,num_layers=best_n_layer)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # optional save\n",
    "    if SAVE_UNET:\n",
    "        torch.save(model, UNET_MODEL_SAVE)\n",
    "        \n",
    "else:\n",
    "    model = torch.load(UNET_MODEL_LOAD)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search of best params for road correction after model prediction or load them from \n",
    "\n",
    "if ROAD_CORRECTION_SEARCH:\n",
    "\n",
    "    # Modulate search\n",
    "    outlier_sizes = np.arange(10, 100, 10)\n",
    "    shape_sizes = np.arange(3, 10)\n",
    "\n",
    "    dataset = Road_data(imgs, gt_imgs)\n",
    "    dataloader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "    threshold = 0.5\n",
    "    best_outlier_size = 0\n",
    "    best_shape_size = 0\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for outlier_size in outlier_sizes:\n",
    "        for shape_size in shape_sizes:\n",
    "\n",
    "            print(f'outlier size : {outlier_size} | shape size : {shape_size}')\n",
    "            c_losses = []\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                gt = target\n",
    "                images = data\n",
    "                pred = model(data)\n",
    "\n",
    "                gt_np = gt.detach().cpu().numpy()\n",
    "                pred_np = pred.detach().cpu().numpy()\n",
    "                test_pred = np.array([\n",
    "                    process_roads(raw_map=raw_map, threshold=threshold, outlier_size=outlier_size, shape_size=shape_size) \n",
    "                    for raw_map in pred_np\n",
    "                    ])\n",
    "                \n",
    "                c_losses.append(np.mean([f1_loss_numpy(p, truth) for p, truth in zip(test_pred,gt_np)]))\n",
    "            \n",
    "            c_loss = np.mean(c_losses)\n",
    "            if c_loss < best_loss:\n",
    "                best_loss = c_loss\n",
    "                best_outlier_size = outlier_size\n",
    "                best_shape_size = shape_size\n",
    "\n",
    "    # optional save\n",
    "    if SAVE_RC_PARAMS:\n",
    "        best_params = {'best_outlier_size':best_outlier_size,'best_shape_size':best_shape_size}\n",
    "        torch.save(best_params, RC_SAVE)\n",
    "\n",
    "else:\n",
    "    best_params = torch.load(RC_LOAD)\n",
    "    best_outlier_size = best_params['best_outlier_size']\n",
    "    best_shape_size = best_params['best_shape_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get plot examples with the trained or loaded UNet model, and train or loaded RC parameters (still on train data)\n",
    "n_examples = 10\n",
    "\n",
    "dataset_check = Road_data(imgs, gt_imgs)\n",
    "dataloader = DataLoader(dataset_check, batch_size=n_examples, shuffle=True)\n",
    "threshold = 0.5\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "    gt = target\n",
    "    pred = model(data)\n",
    "    gt_np = gt.detach().cpu().numpy()\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "\n",
    "    for i in range(n_examples):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(8, 8))\n",
    "\n",
    "        prediction, _, _, _ = process_roads(raw_map=pred_np[i], threshold=threshold, outlier_size=best_outlier_size, shape_size=best_shape_size)\n",
    "        axes[0].imshow(prediction, cmap='gray')\n",
    "        axes[0].set_title(\"Prediction\")\n",
    "        \n",
    "        axes[1].imshow(pred_np[i], cmap='gray')\n",
    "        axes[1].set_title(\"Probability Map\")\n",
    "\n",
    "        axes[2].imshow(gt_np[i], cmap='gray')\n",
    "        axes[2].set_title(\"Ground Truth\")\n",
    "\n",
    "        img_np = data[i].permute(1, 2, 0).cpu().numpy() \n",
    "\n",
    "        axes[3].imshow(img_np)\n",
    "        axes[3].set_title(\"Original Image\")\n",
    "\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].axis(\"off\")\n",
    "        axes[2].axis(\"off\")\n",
    "        axes[3].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_shape_size = 8\n",
    "# best_outlier_size = 80\n",
    "# best_threshold = 0.45 - 0.50\n",
    "# Copy of output\n",
    "'''\n",
    "(2,2)\n",
    "Epoch 10/10, Loss: 0.7271\n",
    "(3,2)\n",
    "Epoch 10/10, Loss: 0.7983\n",
    "(4,2)\n",
    "Epoch 6/10, Loss: 0.8487\n",
    "Epoch 7/10, Loss: 0.8037\n",
    "Epoch 8/10, Loss: 0.7896\n",
    "Epoch 9/10, Loss: 0.8194\n",
    "Epoch 10/10, Loss: 0.8030\n",
    "(5,2)\n",
    "Epoch 10/10, Loss: 0.7603\n",
    "(6,2)\n",
    "Epoch 7/10, Loss: 0.8367\n",
    "Epoch 8/10, Loss: 0.8041\n",
    "Epoch 9/10, Loss: 0.7919\n",
    "Epoch 10/10, Loss: 0.8159\n",
    "(2,4)\n",
    "Epoch 10/10, Loss: 0.7353\n",
    "(3,4)\n",
    "Epoch 10/10, Loss: 0.7317\n",
    "(4,4)\n",
    "Epoch 7/10, Loss: 0.7212\n",
    "Epoch 8/10, Loss: 0.7214\n",
    "Epoch 9/10, Loss: 0.7083\n",
    "Epoch 10/10, Loss: 0.7170\n",
    "(5,4)\n",
    "Epoch 7/10, Loss: 0.7716\n",
    "Epoch 8/10, Loss: 0.7631\n",
    "Epoch 9/10, Loss: 0.7285\n",
    "Epoch 10/10, Loss: 0.7535\n",
    "(6,4)\n",
    "Epoch 7/10, Loss: 0.7280\n",
    "Epoch 8/10, Loss: 0.7524\n",
    "Epoch 9/10, Loss: 0.7306\n",
    "Epoch 10/10, Loss: 0.6984\n",
    "(2,8)\n",
    "Epoch 10/10, Loss: 0.6975\n",
    "(3,8)\n",
    "Epoch 10/10, Loss: 0.7063\n",
    "(4,8)\n",
    "Epoch 10/10, Loss: 0.6899\n",
    "(5,8)\n",
    "Epoch 7/10, Loss: 0.7355\n",
    "Epoch 8/10, Loss: 0.7171\n",
    "Epoch 9/10, Loss: 0.6915\n",
    "Epoch 10/10, Loss: 0.7085\n",
    "(6,8)\n",
    "Epoch 10/10, Loss: 0.6920\n",
    "down\n",
    "(2,16)\n",
    "Epoch 8/10, Loss: 0.6867\n",
    "Epoch 9/10, Loss: 0.6995\n",
    "Epoch 10/10, Loss: 0.6900\n",
    "(3,16)\n",
    "Epoch 7/10, Loss: 0.6969\n",
    "Epoch 8/10, Loss: 0.6711\n",
    "Epoch 9/10, Loss: 0.6747\n",
    "Epoch 10/10, Loss: 0.6778\n",
    "(4,16)\n",
    "Epoch 7/10, Loss: 0.6805\n",
    "Epoch 8/10, Loss: 0.6782\n",
    "Epoch 9/10, Loss: 0.6719\n",
    "Epoch 10/10, Loss: 0.6583\n",
    "(5,16)\n",
    "Epoch 6/10, Loss: 0.7078\n",
    "Epoch 7/10, Loss: 0.7302\n",
    "Epoch 8/10, Loss: 0.6718\n",
    "Epoch 9/10, Loss: 0.6683\n",
    "Epoch 10/10, Loss: 0.6839\n",
    "(6,16)\n",
    "Epoch 6/10, Loss: 0.6901\n",
    "Epoch 7/10, Loss: 0.6790\n",
    "Epoch 8/10, Loss: 0.6811\n",
    "Epoch 9/10, Loss: 0.6777\n",
    "Epoch 10/10, Loss: 0.6808\n",
    "(2,32)\n",
    "Epoch 6/10, Loss: 0.6958\n",
    "Epoch 7/10, Loss: 0.6868\n",
    "Epoch 8/10, Loss: 0.7193\n",
    "Epoch 9/10, Loss: 0.6966\n",
    "Epoch 10/10, Loss: 0.6718\n",
    "(3,32)\n",
    "Epoch 6/10, Loss: 0.7020\n",
    "Epoch 7/10, Loss: 0.6866\n",
    "Epoch 8/10, Loss: 0.6826\n",
    "Epoch 9/10, Loss: 0.6949\n",
    "Epoch 10/10, Loss: 0.6761\n",
    "(4,32)\n",
    "Epoch 7/10, Loss: 0.6746\n",
    "Epoch 8/10, Loss: 0.6920\n",
    "Epoch 9/10, Loss: 0.6757\n",
    "Epoch 10/10, Loss: 0.6643\n",
    "(5,32)\n",
    "Epoch 6/10, Loss: 0.7046\n",
    "Epoch 7/10, Loss: 0.6784\n",
    "Epoch 8/10, Loss: 0.6763\n",
    "Epoch 9/10, Loss: 0.6922\n",
    "Epoch 10/10, Loss: 0.6798\n",
    "(6,32)\n",
    "Epoch 1/10, Loss: 0.7677'''\n",
    "# Crash at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the root directory containing the test image folders\n",
    "test_dir = \"data/test_set_images/\"\n",
    "\n",
    "# Helper function to perform natural sorting\n",
    "def natural_sort_key(s):\n",
    "    # Extract numbers from the string for sorting\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# List all subdirectories (each containing one image) and sort them naturally\n",
    "test_folders = sorted(\n",
    "    [folder for folder in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, folder))],\n",
    "    key=natural_sort_key\n",
    ")\n",
    "\n",
    "# Calculate the number of test images (based on the number of folders)\n",
    "n_test = len(test_folders)\n",
    "print(\"Loading \" + str(n_test) + \" images\")\n",
    "\n",
    "# Iterate over each folder, find the image, and load it\n",
    "test_imgs = []\n",
    "for folder in test_folders:\n",
    "    folder_path = os.path.join(test_dir, folder)\n",
    "    \n",
    "    # List all files in the folder (assuming only one image per folder) and sort them naturally\n",
    "    image_files = sorted(\n",
    "        [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))],\n",
    "        key=natural_sort_key\n",
    "    )\n",
    "    \n",
    "    # Assuming there's only one image per folder, get the image file path\n",
    "    if image_files:\n",
    "        image_path = os.path.join(folder_path, image_files[0])\n",
    "        test_imgs.append(load_image(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "test_imgs_np = np.array(test_imgs)\n",
    "images_test= torch.from_numpy(test_imgs_np).permute(0, 3, 1, 2)\n",
    "pred= model(images_test)\n",
    "pred_np = pred.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
